# -*- coding: utf-8 -*-
"""RAG chatbot PSA.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/11QR1KW_xW8IFliY6NgGhz5vp28RVeajC
"""

from datasets import load_dataset, Dataset, DatasetDict
from sentence_transformers import SentenceTransformer
from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig

import torch, faiss, numpy as np, os, sys, re

class RAGChatbot:
    SYS_PROMPT = """You are an assistant for answering questions.
    You are going to help new incoming pakistani students coming to Germany mostly for masters.

    VERY IMPORTANT : Answer only from the user response / context provided.
    Try to add as little as possible information from your own knowledge. Always prefer the knowledge in the context given.
    Dont mention that the answer is from the context or anything like that. just answer the question directly
    """

    DATA_DIR = "/content/data/split"
    data = None
    dataset = None
    model = None
    tokenizer = None
    terminators = None
    ST = None
    model_id = None

    def __init__(self):
        self.ST = SentenceTransformer("mixedbread-ai/mxbai-embed-large-v1")
        self.model_id = "meta-llama/Meta-Llama-3.1-8B-Instruct"
        self.initialize_model()
        self.dataset = self.read_text_file_as_hfdataset(self.DATA_DIR)
        self.dataset = self.dataset.map(self.embed, batched=True, batch_size=16)
        self.data = self.dataset["train"]
        self.data = self.data.add_faiss_index("embeddings")

    def initialize_model(self):
        bnb_config = BitsAndBytesConfig(
            load_in_4bit=True, bnb_4bit_use_double_quant=True, 
            bnb_4bit_quant_type="nf4", bnb_4bit_compute_dtype=torch.bfloat16
        )
        self.tokenizer = AutoTokenizer.from_pretrained(self.model_id)
        self.model = AutoModelForCausalLM.from_pretrained(
            self.model_id,
            torch_dtype=torch.bfloat16,
            device_map="auto",
            quantization_config=bnb_config
        )
        self.terminators = [
            self.tokenizer.eos_token_id,
            self.tokenizer.convert_tokens_to_ids("<|eot_id|>")
        ]

    def read_text_files(self, data_dir):
        files = os.listdir(data_dir)
        data = []
        for file in files:
            if file.endswith('.txt'):
                with open(os.path.join(data_dir, file), 'r', encoding='utf-8') as f:
                    content = f.read()
                    data.append({"text": content})
        return data

    import re

    def clean_and_format_text(text):
        '''
        This function cleans and formats the text for the user response.
        '''

        # Remove any tags (assuming tags are in the format <tag>)
        cleaned_text = re.sub(r'<[^>]*>', '', text)

        # Extract and format Heading 1
        heading1_match = re.search(r'Heading 1: (.*?)\n', cleaned_text)
        if heading1_match:
            heading1 = heading1_match.group(1).title() + ':'
            cleaned_text = re.sub(r'Heading 1: .*?\n', f'{heading1}\n', cleaned_text)
        else:
            heading1 = ""

        # Extract and format Heading 2
        heading2_match = re.search(r'Heading 2: (.*?)\n', cleaned_text)
        if heading2_match:
            heading2 = heading2_match.group(1).title() + ':'
            cleaned_text = re.sub(r'Heading 2: .*?\n', f'{heading2}\n', cleaned_text)
        else:
            heading2 = ""

        # Extract and format Body text
        body_match = re.search(r'Body:\n(.*)', cleaned_text, re.DOTALL)
        if body_match:
            body = 'Details: ' + body_match.group(1).strip()
        else:
            body = ""

        # Combine all parts
        formatted_text = f'{heading1}\n{heading2}\n{body}'

        return formatted_text.strip()

    def read_text_file_as_hfdataset(self, filepath):
        data = self.read_text_files(filepath)
        dataset = Dataset.from_dict({"text": data})
        dataset = DatasetDict({"train": dataset})
        return dataset

    def embed(self, batch):
        information = batch["text"]
        return {"embeddings": self.ST.encode(information)}

    def search(self, query, k=3):
        embedded_query = self.ST.encode(query)
        scores, retrieved_examples = self.data.get_nearest_examples(
            "embeddings", embedded_query, k=k
        )
        return scores, retrieved_examples

    def format_prompt(self, prompt, retrieved_documents, k):
        PROMPT = f"Question:{prompt}\nContext:"
        for idx in range(k):
            PROMPT += f"{retrieved_documents['text'][idx]}\n"
        return PROMPT

    def generate(self, formatted_prompt):
        messages = [{"role": "system", "content": self.SYS_PROMPT}, {"role": "user", "content": formatted_prompt}]
        input_ids = self.tokenizer.apply_chat_template(
            messages, add_generation_prompt=True, return_tensors="pt"
        ).to(self.model.device)
        outputs = self.model.generate(
            input_ids, max_new_tokens=1024, eos_token_id=self.terminators,
            do_sample=True, temperature=0.6, top_p=0.9,
        )
        response = outputs[0][input_ids.shape[-1]:]
        return self.tokenizer.decode(response, skip_special_tokens=True)

    def rag_chatbot(self, prompt, k=3):
        scores, retrieved_documents = self.search(prompt, k)
        formatted_prompt = self.format_prompt(prompt, retrieved_documents, k)
        return self.generate(formatted_prompt)
